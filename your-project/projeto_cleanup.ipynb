{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import nltk\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to prepare dataset ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, stemming, tagging, removing stopwords\n",
    "def clean_up(s):\n",
    "    words = re.findall('[^\\d\\W]+', str(s))#, ' '.join(s))\n",
    "    #words = words.split()\n",
    "    words = [w.lower() for w in words if not w.startswith('http://') and len(w)>2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    stop_words.append('ii')\n",
    "    stop_words.append('iii')\n",
    "    stop_words.append('iv')\n",
    "    stop_words.append('º')\n",
    "    stop_words.append('ª')\n",
    "    stop_words.append('nº')\n",
    "    return [word for word in l if word not in stop_words]\n",
    "\n",
    "# here we try stem and pos_tag to check, the lemmatization does not work well in portuguese\n",
    "## the pos tag was trainned below as 'tagger' with a specific algorithm for portuguese language\n",
    "def tag_stem(l):\n",
    "    l = ' '.join(l)\n",
    "    tagged = tagger.tag(nltk.word_tokenize(l))\n",
    "    words, tags = zip(*tagged)\n",
    "    stem = [RSLPStemmer().stem(word) for word in words]\n",
    "    return list(zip(stem,tags))\n",
    "\n",
    "# applying all\n",
    "def set_up(x):\n",
    "    return x.apply(clean_up).apply(word_tokenize).apply(remove_stopwords).apply(tag_stem)\n",
    "\n",
    "# this funcion will clean the english tags to be portuguese friendly\n",
    "def convert_to_universal_tag(t, reverse=False):\n",
    "    tagdict = {\n",
    "        'n': \"NOUN\",\n",
    "        'num': \"NUM\",\n",
    "        'v-fin': \"VERB\",\n",
    "        'v-inf': \"VERB\",\n",
    "        'v-ger': \"VERB\",\n",
    "        'v-pcp': \"VERB\",\n",
    "        'pron-det': \"PRON\",\n",
    "        'pron-indp': \"PRON\",\n",
    "        'pron-pers': \"PRON\",\n",
    "        'art': \"DET\",\n",
    "        'adv': \"ADV\",\n",
    "        'conj-s': \"CONJ\",\n",
    "        'conj-c': \"CONJ\",\n",
    "        'conj-p': \"CONJ\",\n",
    "        'adj': \"ADJ\",\n",
    "        'ec': \"PRT\",\n",
    "        'pp': \"ADP\",\n",
    "        'prp': \"ADP\",\n",
    "        'prop': \"NOUN\",\n",
    "        'pro-ks-rel': \"PRON\",\n",
    "        'proadj': \"PRON\",\n",
    "        'prep': \"ADP\",\n",
    "        'nprop': \"NOUN\",\n",
    "        'vaux': \"VERB\",\n",
    "        'propess': \"PRON\",\n",
    "        'v': \"VERB\",\n",
    "        'vp': \"VERB\",\n",
    "        'in': \"X\",\n",
    "        'prp-': \"ADP\",\n",
    "        'adv-ks': \"ADV\",\n",
    "        'dad': \"NUM\",\n",
    "        'prosub': \"PRON\",\n",
    "        'tel': \"NUM\",\n",
    "        'ap': \"NUM\",\n",
    "        'est': \"NOUN\",\n",
    "        'cur': \"X\",\n",
    "        'pcp': \"VERB\",\n",
    "        'pro-ks': \"PRON\",\n",
    "        'hor': \"NUM\",\n",
    "        'pden': \"ADV\",\n",
    "        'dat': \"NUM\",\n",
    "        'kc': \"ADP\",\n",
    "        'ks': \"ADP\",\n",
    "        'adv-ks-rel': \"ADV\",\n",
    "        'npro': \"NOUN\",\n",
    "    }\n",
    "    if t in [\"N|AP\",\"N|DAD\",\"N|DAT\",\"N|HOR\",\"N|TEL\"]:\n",
    "        t = \"NUM\"\n",
    "    if reverse:\n",
    "        if \"|\" in t: t = t.split(\"|\")[0]\n",
    "    else:\n",
    "        if \"+\" in t: t = t.split(\"+\")[1]\n",
    "        if \"|\" in t: t = t.split(\"|\")[1]\n",
    "        if \"#\" in t: t = t.split(\"#\")[0]\n",
    "    t = t.lower()\n",
    "    return tagdict.get(t, \".\" if all(tt in punctuation for tt in t) else t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "def bow(x, n=5000):\n",
    "    allwords = [w for words in x for w in words if len(w) > 1]\n",
    "    bag = {k:allwords.count(k) for k in allwords}\n",
    "    sorted_bag = sorted(bag.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    sb = {k:v for k,v in sorted_bag[:n]}\n",
    "    return pd.DataFrame(sb, index=['values'])\n",
    "\n",
    "# check if words are in text\n",
    "def find_features(document):\n",
    "    word = set(document)\n",
    "    return {w:(w in word) for w in words.columns}\n",
    "\n",
    "# multiply two lists\n",
    "def mult(a,b, c=[]):\n",
    "    for i in range(len(a)):\n",
    "        c.append(a[i]*b[i])\n",
    "    return c\n",
    "\n",
    "# separating words from tags\n",
    "def words(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [word for word in words]\n",
    "\n",
    "def tags(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [tag.lower() for tag in tags]\n",
    "\n",
    "# adding weights to each\n",
    "def weight(l, d):\n",
    "    res = []\n",
    "    for i in l:\n",
    "        try:\n",
    "            res.append(d[i])\n",
    "        except:\n",
    "            res.append(0)\n",
    "    return res\n",
    "\n",
    "# assign weights for vectors\n",
    "def assign_weights(a, b, c=[]):\n",
    "    for i in range(len(a)):\n",
    "        return [(a[i] * np.array(b[i])).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing dataset and getting only unique values\n",
    "df = pd.read_csv('decisions.csv', index_col='Unnamed: 0')\n",
    "uniq = pd.DataFrame()\n",
    "uniq['main'] = df['Main Judgement'].unique()\n",
    "uniq['main'] = uniq['main'].dropna()\n",
    "uniq['len'] = uniq['main'].apply(str).apply(len)\n",
    "uniq = uniq[uniq['len'] > 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
