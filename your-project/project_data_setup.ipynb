{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import nltk\n",
    "import re \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning, stemming, tagging, removing stopwords\n",
    "def clean_up(s):\n",
    "    words = re.findall('[^\\d\\W]+', str(s))#, ' '.join(s))\n",
    "    #words = words.split()\n",
    "    words = [w.lower() for w in words if not w.startswith('http://') and len(w)>2]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    stop_words = stopwords.words('portuguese')\n",
    "    stop_words.append('ii')\n",
    "    stop_words.append('iii')\n",
    "    stop_words.append('iv')\n",
    "    stop_words.append('º')\n",
    "    stop_words.append('ª')\n",
    "    stop_words.append('nº')\n",
    "    return [word for word in l if word not in stop_words]\n",
    "\n",
    "# here we try stem and pos_tag to check, the lemmatization does not work well in portuguese\n",
    "## the pos tag was trainned below as 'tagger' with a specific algorithm for portuguese language\n",
    "def tag_stem(l):\n",
    "    l = ' '.join(l)\n",
    "    tagged = tagger.tag(nltk.word_tokenize(l))\n",
    "    words, tags = zip(*tagged)\n",
    "    stem = [RSLPStemmer().stem(word) for word in words]\n",
    "    return list(zip(stem,tags))\n",
    "\n",
    "# applying all\n",
    "def set_up(x):\n",
    "    return x.apply(clean_up).apply(word_tokenize).apply(remove_stopwords).apply(tag_stem)\n",
    "\n",
    "# this funcion will clean the english tags to be portuguese friendly\n",
    "def convert_to_universal_tag(t, reverse=False):\n",
    "    tagdict = {\n",
    "        'n': \"NOUN\",\n",
    "        'num': \"NUM\",\n",
    "        'v-fin': \"VERB\",\n",
    "        'v-inf': \"VERB\",\n",
    "        'v-ger': \"VERB\",\n",
    "        'v-pcp': \"VERB\",\n",
    "        'pron-det': \"PRON\",\n",
    "        'pron-indp': \"PRON\",\n",
    "        'pron-pers': \"PRON\",\n",
    "        'art': \"DET\",\n",
    "        'adv': \"ADV\",\n",
    "        'conj-s': \"CONJ\",\n",
    "        'conj-c': \"CONJ\",\n",
    "        'conj-p': \"CONJ\",\n",
    "        'adj': \"ADJ\",\n",
    "        'ec': \"PRT\",\n",
    "        'pp': \"ADP\",\n",
    "        'prp': \"ADP\",\n",
    "        'prop': \"NOUN\",\n",
    "        'pro-ks-rel': \"PRON\",\n",
    "        'proadj': \"PRON\",\n",
    "        'prep': \"ADP\",\n",
    "        'nprop': \"NOUN\",\n",
    "        'vaux': \"VERB\",\n",
    "        'propess': \"PRON\",\n",
    "        'v': \"VERB\",\n",
    "        'vp': \"VERB\",\n",
    "        'in': \"X\",\n",
    "        'prp-': \"ADP\",\n",
    "        'adv-ks': \"ADV\",\n",
    "        'dad': \"NUM\",\n",
    "        'prosub': \"PRON\",\n",
    "        'tel': \"NUM\",\n",
    "        'ap': \"NUM\",\n",
    "        'est': \"NOUN\",\n",
    "        'cur': \"X\",\n",
    "        'pcp': \"VERB\",\n",
    "        'pro-ks': \"PRON\",\n",
    "        'hor': \"NUM\",\n",
    "        'pden': \"ADV\",\n",
    "        'dat': \"NUM\",\n",
    "        'kc': \"ADP\",\n",
    "        'ks': \"ADP\",\n",
    "        'adv-ks-rel': \"ADV\",\n",
    "        'npro': \"NOUN\",\n",
    "    }\n",
    "    if t in [\"N|AP\",\"N|DAD\",\"N|DAT\",\"N|HOR\",\"N|TEL\"]:\n",
    "        t = \"NUM\"\n",
    "    if reverse:\n",
    "        if \"|\" in t: t = t.split(\"|\")[0]\n",
    "    else:\n",
    "        if \"+\" in t: t = t.split(\"+\")[1]\n",
    "        if \"|\" in t: t = t.split(\"|\")[1]\n",
    "        if \"#\" in t: t = t.split(\"#\")[0]\n",
    "    t = t.lower()\n",
    "    return tagdict.get(t, \".\" if all(tt in punctuation for tt in t) else t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words\n",
    "def bow(x, n=5000):\n",
    "    allwords = [w for words in x for w in words if len(w) > 1]\n",
    "    bag = {k:allwords.count(k) for k in allwords}\n",
    "    sorted_bag = sorted(bag.items(), key=lambda kv: kv[1], reverse=True)\n",
    "    sb = {k:v for k,v in sorted_bag[:n]}\n",
    "    return pd.DataFrame(sb, index=['values'])\n",
    "\n",
    "# check if words are in text\n",
    "def find_features(document):\n",
    "    word = set(document)\n",
    "    return {w:(w in word) for w in words.columns}\n",
    "\n",
    "# multiply two lists\n",
    "def mult(a,b, c=[]):\n",
    "    for i in range(len(a)):\n",
    "        c.append(a[i]*b[i])\n",
    "    return c\n",
    "\n",
    "# separating words from tags\n",
    "def words(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [word for word in words]\n",
    "\n",
    "def tags(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [tag.lower() for tag in tags]\n",
    "\n",
    "# adding weights to each\n",
    "def weight(l, d):\n",
    "    res = []\n",
    "    for i in l:\n",
    "        try:\n",
    "            res.append(d[i])\n",
    "        except:\n",
    "            res.append(0)\n",
    "    return res\n",
    "\n",
    "# assign weights for vectors\n",
    "def assign_weights(a, b, c=[]):\n",
    "    for i in range(len(a)):\n",
    "        return [(a[i] * np.array(b[i])).tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model for Portuguese POS tagging\n",
    "For lemmatization to work well in portuguese language and to avoid ambiguity, we need to train the model for recognizing and tagging by the Part-of-Speech method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ef7ca7c93d13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# nltk.corpus.mac_morpho.tagged_sents is incorrect, converting tagged_paras to tagged_sents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdataset1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloresta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdataset2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmac_morpho\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagged_paras\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtraindata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_to_universal_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;31m# iterate_from() sets self._len when it reaches the end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             \u001b[1;31m# of the file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_toknum\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_toknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoknum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_blocknum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock_index\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 306\u001b[1;33m             \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    307\u001b[0m             assert isinstance(tokens, (tuple, list, AbstractLazySequence)), (\n\u001b[0;32m    308\u001b[0m                 \u001b[1;34m'block reader %s() should return list or tuple.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mreader\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtagged_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_tagged_sent_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m         return concat(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36m_read_tagged_sent_block\u001b[1;34m(self, stream, tagset)\u001b[0m\n\u001b[0;32m    475\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_tagged_sent_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         return list(\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         )\n\u001b[0;32m    479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\bracket_parse.py\u001b[0m in \u001b[0;36m_read_block\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_detect_blocks\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'unindented_paren'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;31m# Tokens start with unindented left parens.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mtoks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_regexp_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_re\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mr'^\\('\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;31m# Strip any comments out of the tokens.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_comment_char\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36mread_regexp_block\u001b[1;34m(stream, start_re, end_re)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 657\u001b[1;33m         \u001b[0moldpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    658\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m         \u001b[1;31m# End of file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mtell\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;31m# Store our original file position, so we can return here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1406\u001b[1;33m         \u001b[0morig_filepos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1407\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1408\u001b[0m         \u001b[1;31m# Calculate an estimate of where we think the newline is.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# nltk.corpus.mac_morpho.tagged_sents is incorrect, converting tagged_paras to tagged_sents\n",
    "dataset1 = list(nltk.corpus.floresta.tagged_sents())\n",
    "dataset2 = [[w[0] for w in sent] for sent in nltk.corpus.mac_morpho.tagged_paras()]\n",
    "\n",
    "traindata = [[(w, convert_to_universal_tag(t)) for (w, t) in sent] for sent in dataset1]\n",
    "traindata2 = traindata + [[(w, convert_to_universal_tag(t, reverse=True)) for (w, t) in sent]\n",
    "                          for sent in dataset2]\n",
    "\n",
    "shuffle(traindata)\n",
    "shuffle(traindata2)\n",
    "\n",
    "regex_patterns = [(r\"^[nN][ao]s?$\", \"ADP\"), (r\"^[dD][ao]s?$\", \"ADP\"), (r\"^[pP]el[ao]s?$\", \"ADP\"),\n",
    "                  (r\"^[nN]est[ae]s?$\", \"ADP\"), (r\"^[nN]um$\", \"ADP\"), (r\"^[nN]ess[ae]s?$\", \"ADP\"),\n",
    "                  (r\"^[nN]aquel[ae]s?$\", \"ADP\"), (r\"^\\xe0$\", \"ADP\")]\n",
    "\n",
    "# training model and getting tagger ready\n",
    "tagger = nltk.BigramTagger(\n",
    "            traindata, backoff=nltk.RegexpTagger(\n",
    "                regex_patterns, backoff=nltk.UnigramTagger(\n",
    "                    traindata2, backoff=nltk.AffixTagger(\n",
    "                        traindata2, backoff=nltk.DefaultTagger('NOUN')))))\n",
    "\n",
    "templates = nltk.brill.fntbl37()\n",
    "tagger = nltk.BrillTaggerTrainer(tagger, templates)\n",
    "tagger = tagger.train(traindata, max_rules=100)\n",
    "\n",
    "# saving model\n",
    "f = open('tagger.pickle', 'wb')\n",
    "pickle.dump(tagger, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading pos_tags model for portuguese language\n",
    "f = open('tagger.pickle', 'rb')\n",
    "tagger = pickle.load(f)\n",
    "f.close()\n",
    "# getting tokenized and applying predetermined functions\n",
    "tokens_save = set_up(uniq['main'])\n",
    "tokens_save.head()\n",
    "\n",
    "with open(\"tokens.json\", \"w\") as f:\n",
    "    json.dump(tokens_save.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tokens.json') as json_data:\n",
    "    data = json.load(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['main'] = data.values()\n",
    "features['words'] = features['main'].apply(words)\n",
    "features['tags'] = features['main'].apply(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
