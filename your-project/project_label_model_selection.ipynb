{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separating words from tags\n",
    "def words(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [word for word in words]\n",
    "\n",
    "def tags(l):\n",
    "    words, tags = zip(*l)\n",
    "    return [tag.lower() for tag in tags]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Dataframe\n",
    "Vectorizing all the words and tags, labelling positive/negative and validating with supervised classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing\n",
    "with open('tokens.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    \n",
    "features = pd.DataFrame()\n",
    "features['main'] = data.values()\n",
    "features['words'] = features['main'].apply(words).apply(' '.join)\n",
    "features['tags'] = features['main'].apply(tags).apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polarity Checking for Positive/Negative Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# averaging words more common in negative rullings\n",
    "lengths = features['words'].apply(lambda x: len(x.split()))\n",
    "\n",
    "polar = np.diag(features['main'].apply(lambda x: (len(re.findall('nem', str(x)))+\\\n",
    "                                              len(re.findall('não', str(x)))+\\\n",
    "                                              len(re.findall('inexiste', str(x)))+\\\n",
    "                                              len(re.findall('negado', str(x)))+\\\n",
    "                                              len(re.findall('deveria', str(x)))+\\\n",
    "                                              len(re.findall('falta', str(x)))+\\\n",
    "                                              len(re.findall('demonstre', str(x)))+\\\n",
    "                                              len(re.findall('improced.+', str(x)))*1.5+\\\n",
    "                                              len(re.findall('insucesso', str(x))))/lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9822"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['polarity'] = [1 if p <= 0.03 else 0 for p in polar]\n",
    "features['polarity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Supervised Learning \n",
    "Using classification to validate the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features['words'])\n",
    "y = np.array(features['polarity'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   30.9s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=3)]: Done  19 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=3)]: Done  26 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=3)]: Done  35 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=3)]: Done  55 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=3)]: Done  66 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=3)]: Done  72 out of  72 | elapsed:  2.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       478\n",
      "           1       0.67      0.68      0.68       507\n",
      "\n",
      "    accuracy                           0.66       985\n",
      "   macro avg       0.66      0.66      0.66       985\n",
      "weighted avg       0.66      0.66      0.66       985\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.63451777, 0.65228426, 0.65126904, 0.64213198, 0.65396341])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LogisticRegression\n",
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(\n",
    "                            LogisticRegression(), n_jobs=1))])\n",
    "\n",
    "parameters = {\"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None],\n",
    "                     \"clf__estimator__solver\": ['saga', 'lbfgs'],\n",
    "                          \"clf__estimator__max_iter\": [100, 500]\n",
    "             }\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=3, verbose=10)\n",
    "grid_search_tune.fit(X_train,y_train)\n",
    "best_log = grid_search_tune.best_estimator_\n",
    "\n",
    "y_pred_log = best_log.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_log))\n",
    "\n",
    "cross_val_score(best_log, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   16.3s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  18 | elapsed:   25.5s remaining:    5.0s\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:   30.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.63      0.64       478\n",
      "           1       0.67      0.69      0.68       507\n",
      "\n",
      "    accuracy                           0.66       985\n",
      "   macro avg       0.66      0.66      0.66       985\n",
      "weighted avg       0.66      0.66      0.66       985\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.63451777, 0.64873096, 0.65228426, 0.64263959, 0.65142276])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LinearSVC\n",
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(\n",
    "                            LinearSVC(), n_jobs=1))])\n",
    "\n",
    "parameters = {\"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None]\n",
    "             }\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=3, verbose=10)\n",
    "grid_search_tune.fit(X_train,y_train)\n",
    "best_svc = grid_search_tune.best_estimator_\n",
    "\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "cross_val_score(best_svc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__estimator__C': 0.1, 'clf__estimator__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_tune.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For better results on finding negative rullings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar = np.diag(features['main'].apply(lambda x: (len(re.findall('nem', x))*1.2+\\\n",
    "                                              len(re.findall('não', x))+\\\n",
    "                                              len(re.findall('inexiste', x))*1.2+\\\n",
    "                                              len(re.findall('negado', x))*1.2+\\\n",
    "                                              len(re.findall('deveria', x))+\\\n",
    "                                              len(re.findall('falta', x))*1.2+\\\n",
    "                                              len(re.findall('demonstre', x))*+\\\n",
    "                                              len(re.findall('improced.+', x))*1.5+\\\n",
    "                                              len(re.findall('insucesso', x))*1.2)/lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['neg_polarity'] = [1 if p <= 0.025 else 0 for p in polar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:   12.4s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   24.4s\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  18 | elapsed:   28.5s remaining:    5.6s\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:   33.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.83      0.73       874\n",
      "           1       0.59      0.36      0.45       604\n",
      "\n",
      "    accuracy                           0.64      1478\n",
      "   macro avg       0.62      0.59      0.59      1478\n",
      "weighted avg       0.63      0.64      0.62      1478\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.65076142, 0.64923858, 0.65482234, 0.65718639, 0.65109192])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(features['words'])\n",
    "y = np.array(features['neg_polarity'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(\n",
    "                            LinearSVC(), n_jobs=1))])\n",
    "\n",
    "parameters = {\"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None]\n",
    "             }\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=3, verbose=10)\n",
    "grid_search_tune.fit(X_train,y_train)\n",
    "best_svc = grid_search_tune.best_estimator_\n",
    "\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "cross_val_score(best_svc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['neg_polarity'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:   16.6s\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  18 | elapsed:   18.8s remaining:    3.7s\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:   20.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.37      0.47       243\n",
      "           1       0.67      0.86      0.75       357\n",
      "\n",
      "    accuracy                           0.66       600\n",
      "   macro avg       0.66      0.61      0.61       600\n",
      "weighted avg       0.66      0.66      0.64       600\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.66527893, 0.67110741, 0.6675    , 0.63886572, 0.58298582])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1 = features[features['neg_polarity'] == 1]['words'][:3000]\n",
    "y1 = features[features['neg_polarity'] == 1]['polarity'][:3000]\n",
    "X2 = features[features['neg_polarity'] == 0]['words'][:3000]\n",
    "y2 = features[features['neg_polarity'] == 0]['polarity'][:3000]\n",
    "\n",
    "X = pd.concat([X1,X2])\n",
    "y = pd.concat([y1,y2])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=17)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(\n",
    "                            LinearSVC(), n_jobs=1))])\n",
    "\n",
    "parameters = {\"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None]\n",
    "             }\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=3, verbose=10)\n",
    "grid_search_tune.fit(X_train,y_train)\n",
    "best_nsvc = grid_search_tune.best_estimator_\n",
    "\n",
    "y_pred_nsvc = best_nsvc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_nsvc))\n",
    "\n",
    "cross_val_score(best_nsvc, X, y, cv=5)\n",
    "# good model for predicting positive sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting Labelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reindex from a duplicate axis",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-09d12ecc152d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'main'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muniq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'main'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'area'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Area'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"labelled.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3486\u001b[0m             \u001b[1;31m# set column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3487\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   3562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3563\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3564\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3565\u001b[0m         \u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[1;34m(self, key, value, broadcast)\u001b[0m\n\u001b[0;32m   3724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3726\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreindexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3728\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   3715\u001b[0m                     \u001b[1;31m# duplicate axis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3716\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3717\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3719\u001b[0m                     \u001b[1;31m# other\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mreindexer\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   3710\u001b[0m                 \u001b[1;31m# GH 4107\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3711\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3712\u001b[1;33m                     \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3713\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, index, **kwargs)\u001b[0m\n\u001b[0;32m   4219\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4221\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4223\u001b[0m     def drop(\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4512\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4513\u001b[0m         return self._reindex_axes(\n\u001b[1;32m-> 4514\u001b[1;33m             \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4515\u001b[0m         ).__finalize__(self)\n\u001b[0;32m   4516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   4533\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4534\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4535\u001b[1;33m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4536\u001b[0m             )\n\u001b[0;32m   4537\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   4575\u001b[0m                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4576\u001b[0m                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4577\u001b[1;33m                 \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4578\u001b[0m             )\n\u001b[0;32m   4579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[0;32m   1249\u001b[0m         \u001b[1;31m# some axes don't allow reindexing with dups\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_reindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m_can_reindex\u001b[1;34m(self, indexer)\u001b[0m\n\u001b[0;32m   3360\u001b[0m         \u001b[1;31m# trying to reindex on an axis with duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3361\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3362\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot reindex from a duplicate axis\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reindex from a duplicate axis"
     ]
    }
   ],
   "source": [
    "features['main'] = uniq['main']\n",
    "\n",
    "with open(\"labelled.json\", \"w\") as f:\n",
    "    json.dump(features.to_dict(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearSVC\n",
    "with open('tag_stems_topics.json') as json_data:\n",
    "    data_stem = json.load(json_data)\n",
    "    \n",
    "stemmed = {k:' '.join(v) for k,v in data_stem.items()}\n",
    "df_stem = pd.DataFrame.from_dict(stemmed, orient='index')\n",
    "words_stem = pd.Series(df_stem[0].apply(lambda x: x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan], dtype=object)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(features['new'])\n",
    "y = np.array(features['polarity'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=3)]: Using backend LokyBackend with 3 concurrent workers.\n",
      "[Parallel(n_jobs=3)]: Done   2 tasks      | elapsed:    4.1s\n",
      "[Parallel(n_jobs=3)]: Done   7 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=3)]: Done  12 tasks      | elapsed:    9.2s\n",
      "[Parallel(n_jobs=3)]: Done  15 out of  18 | elapsed:   11.0s remaining:    2.1s\n",
      "[Parallel(n_jobs=3)]: Done  18 out of  18 | elapsed:   12.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.33      0.33         3\n",
      "           1       1.00      1.00      1.00      1475\n",
      "\n",
      "    accuracy                           1.00      1478\n",
      "   macro avg       0.67      0.67      0.67      1478\n",
      "weighted avg       1.00      1.00      1.00      1478\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99543379, 0.99796954, 0.99796851, 0.99847638, 0.99746064])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "                    ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', OneVsRestClassifier(\n",
    "                            LinearSVC(), n_jobs=1))])\n",
    "\n",
    "parameters = {\"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None]\n",
    "             }\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=3, n_jobs=3, verbose=10)\n",
    "grid_search_tune.fit(X_train,y_train)\n",
    "best_svc = grid_search_tune.best_estimator_\n",
    "\n",
    "y_pred_svc = best_svc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred_svc))\n",
    "\n",
    "cross_val_score(best_svc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       acord despach profer aut pres cas decis conden...\n",
       "1       def pres reclam confer admit recurs circunscri...\n",
       "2       estabelec hospit contribu trat assist vítim ti...\n",
       "3       form admit revist excepc fundament art cpc rel...\n",
       "4       result artig ccivil val bem do mesm dat abert ...\n",
       "                              ...                        \n",
       "9843    matér fact pod ser alter stj verif algum funda...\n",
       "9844    sed sane sentenç juiz dev term art cpc começ c...\n",
       "9845    decid atribu relev caus exclus culp aleg ré ag...\n",
       "9846    stj pod censur mau uso tribun relaç event feit...\n",
       "9847    constitu associ particip situ alguém exerc act...\n",
       "Name: words, Length: 9848, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       acord despach prof aut pre ca decil conden lit...\n",
       "1       def pre conf admit recur circunscrit aleg viol...\n",
       "2       estabelec hospit contribu trat assist vítim ti...\n",
       "3       form admit revist excepc fundament art cpc rel...\n",
       "4       result artig ccivil val mesm dat abert suces v...\n",
       "                              ...                        \n",
       "9843    matér fact pod stj verif fundament previst par...\n",
       "9844    sed san sentenç juiz dev term art cpc começ co...\n",
       "9845    decid atribu relev cau exclu culp aleg convicç...\n",
       "9846    stj pod censur mau uso tribun relaç event feit...\n",
       "9847    constitu assoc particip situ exerc activ econó...\n",
       "Name: 0, Length: 9848, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_stem.apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [[acord, NOUN], [despach, NOUN], [profer, VERB...\n",
       "1       [[def, VERB], [pres, ADJ], [reclam, NOUN], [co...\n",
       "2       [[estabelec, NOUN], [hospit, NOUN], [contribu,...\n",
       "3       [[form, NOUN], [admit, VERB], [revist, NOUN], ...\n",
       "4       [[result, VERB], [artig, NOUN], [ccivil, NOUN]...\n",
       "                              ...                        \n",
       "9843    [[matér, NOUN], [fact, NOUN], [pod, VERB], [se...\n",
       "9844    [[sed, NOUN], [sane, NOUN], [sentenç, NOUN], [...\n",
       "9845    [[decid, VERB], [atribu, VERB], [relev, NOUN],...\n",
       "9846    [[stj, NOUN], [pod, VERB], [censur, VERB], [ma...\n",
       "9847    [[constitu, VERB], [associ, NOUN], [particip, ...\n",
       "Name: main, Length: 9848, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['main']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['new'] = words_stem.apply(' '.join).reindex(range(len(words_stem))).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>main</th>\n",
       "      <th>words</th>\n",
       "      <th>tags</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[[acord, NOUN], [despach, NOUN], [profer, VERB...</td>\n",
       "      <td>acord despach profer aut pres cas decis conden...</td>\n",
       "      <td>noun noun verb noun adj noun noun noun noun ve...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[[def, VERB], [pres, ADJ], [reclam, NOUN], [co...</td>\n",
       "      <td>def pres reclam confer admit recurs circunscri...</td>\n",
       "      <td>verb adj noun noun verb noun verb noun verb no...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[[estabelec, NOUN], [hospit, NOUN], [contribu,...</td>\n",
       "      <td>estabelec hospit contribu trat assist vítim ti...</td>\n",
       "      <td>noun noun verb noun noun noun adj noun noun no...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[[form, NOUN], [admit, VERB], [revist, NOUN], ...</td>\n",
       "      <td>form admit revist excepc fundament art cpc rel...</td>\n",
       "      <td>noun verb noun adj noun noun noun adv noun ver...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[[result, VERB], [artig, NOUN], [ccivil, NOUN]...</td>\n",
       "      <td>result artig ccivil val bem do mesm dat abert ...</td>\n",
       "      <td>verb noun noun noun noun verb pron noun noun n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9843</td>\n",
       "      <td>[[matér, NOUN], [fact, NOUN], [pod, VERB], [se...</td>\n",
       "      <td>matér fact pod ser alter stj verif algum funda...</td>\n",
       "      <td>noun noun verb verb verb noun verb pron noun v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9844</td>\n",
       "      <td>[[sed, NOUN], [sane, NOUN], [sentenç, NOUN], [...</td>\n",
       "      <td>sed sane sentenç juiz dev term art cpc começ c...</td>\n",
       "      <td>noun noun noun noun verb noun noun noun verb v...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9845</td>\n",
       "      <td>[[decid, VERB], [atribu, VERB], [relev, NOUN],...</td>\n",
       "      <td>decid atribu relev caus exclus culp aleg ré ag...</td>\n",
       "      <td>verb verb noun verb noun noun verb noun verb n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9846</td>\n",
       "      <td>[[stj, NOUN], [pod, VERB], [censur, VERB], [ma...</td>\n",
       "      <td>stj pod censur mau uso tribun relaç event feit...</td>\n",
       "      <td>noun verb verb adj noun noun noun adv verb nou...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9847</td>\n",
       "      <td>[[constitu, VERB], [associ, NOUN], [particip, ...</td>\n",
       "      <td>constitu associ particip situ alguém exerc act...</td>\n",
       "      <td>verb noun noun noun pron verb noun adj noun no...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9848 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   main  \\\n",
       "0     [[acord, NOUN], [despach, NOUN], [profer, VERB...   \n",
       "1     [[def, VERB], [pres, ADJ], [reclam, NOUN], [co...   \n",
       "2     [[estabelec, NOUN], [hospit, NOUN], [contribu,...   \n",
       "3     [[form, NOUN], [admit, VERB], [revist, NOUN], ...   \n",
       "4     [[result, VERB], [artig, NOUN], [ccivil, NOUN]...   \n",
       "...                                                 ...   \n",
       "9843  [[matér, NOUN], [fact, NOUN], [pod, VERB], [se...   \n",
       "9844  [[sed, NOUN], [sane, NOUN], [sentenç, NOUN], [...   \n",
       "9845  [[decid, VERB], [atribu, VERB], [relev, NOUN],...   \n",
       "9846  [[stj, NOUN], [pod, VERB], [censur, VERB], [ma...   \n",
       "9847  [[constitu, VERB], [associ, NOUN], [particip, ...   \n",
       "\n",
       "                                                  words  \\\n",
       "0     acord despach profer aut pres cas decis conden...   \n",
       "1     def pres reclam confer admit recurs circunscri...   \n",
       "2     estabelec hospit contribu trat assist vítim ti...   \n",
       "3     form admit revist excepc fundament art cpc rel...   \n",
       "4     result artig ccivil val bem do mesm dat abert ...   \n",
       "...                                                 ...   \n",
       "9843  matér fact pod ser alter stj verif algum funda...   \n",
       "9844  sed sane sentenç juiz dev term art cpc começ c...   \n",
       "9845  decid atribu relev caus exclus culp aleg ré ag...   \n",
       "9846  stj pod censur mau uso tribun relaç event feit...   \n",
       "9847  constitu associ particip situ alguém exerc act...   \n",
       "\n",
       "                                                   tags  new  \n",
       "0     noun noun verb noun adj noun noun noun noun ve...  NaN  \n",
       "1     verb adj noun noun verb noun verb noun verb no...  NaN  \n",
       "2     noun noun verb noun noun noun adj noun noun no...  NaN  \n",
       "3     noun verb noun adj noun noun noun adv noun ver...  NaN  \n",
       "4     verb noun noun noun noun verb pron noun noun n...  NaN  \n",
       "...                                                 ...  ...  \n",
       "9843  noun noun verb verb verb noun verb pron noun v...  NaN  \n",
       "9844  noun noun noun noun verb noun noun noun verb v...  NaN  \n",
       "9845  verb verb noun verb noun noun verb noun verb n...  NaN  \n",
       "9846  noun verb verb adj noun noun noun adv verb nou...  NaN  \n",
       "9847  verb noun noun noun pron verb noun adj noun no...  NaN  \n",
       "\n",
       "[9848 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(RangeIndex(start=0, stop=9848, step=1),\n",
       " RangeIndex(start=0, stop=9848, step=1))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.index, words_stem.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9848"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_stem.apply(' '.join).reindex(range(len(words_stem))).to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9848"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_stem = words_stem.apply(' '.join).reindex(range(len(words_stem)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
